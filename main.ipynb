{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
    "<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Raleway:wght@100&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<h1 style=\"text-transform: uppercase; text-align: center; font-weight: 100;\">\n",
    "Linear regression in Tensorflow using RNN</h1>\n",
    "<p style=\"text-align: center; font-weight: 100;\">Predicting Volkswagen car prices</p><br/>\n",
    "\n",
    "Project requirements:\n",
    "- Only requirement is: minimum 10k rows\n",
    "- Any type of neural network - classical but convolutional and recurrent, better grade\n",
    "- Presentation of project, beside practical questions about model there would be also theoretical.\n",
    "The topic is free.\n",
    "\n",
    "Theory required for project presentation:\n",
    "- Practical side.\n",
    "- Discuss the scheme of an artificial neuron.\n",
    "- Activation function, why it is so important.\n",
    "- Explain how does neuron learn - steps, algorithm?\n",
    "- How does basic neural network work? (The more you know the better, as well as more complex).\n",
    "- Discuss the algorithm of neural network using backpropagation learning method. (metoda wstecznej propagacji błędów)\n",
    "- What subsets and why are the data divided into?\n",
    "Knowledge of ML, statistics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data source: data were taken from the popular Polish automotive website Otomoto (https://www.otomoto.pl/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Import libraries and create dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn as sl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels as sm\n",
    "import tensorflow as tf\n",
    "import keras.utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# from model import VolkswagenModel\n",
    "# from colorama import init, Fore, Style\n",
    "\n",
    "print(f\"numpy: {np.__version__}, pandas: {pd.__version__}, tensorflow: {tf.__version__}, matplotlib: {mpl.__version__}, seaborn: {sns.__version__}, statsmodels: {sm.__version__}, sklearn: {sl.__version__}\")\n",
    "print(\"Libs loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/otomoto.csv\")\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "df.columns = ['Price', 'Year', 'Mileage', 'Tank capacity', 'Fuel type', 'Model', 'Estimation']\n",
    "print(\"Data technical info\")\n",
    "df.info()\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "description = df.describe()\n",
    "description"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "check = 14915\n",
    "flag = True\n",
    "for i in range(description.shape[1]):\n",
    "    if description.iloc[0, i] != check:\n",
    "        print(\"Number of occurrences of data is not equal for every label.\")\n",
    "        print(f\"Problem at cell: (0, {i})\")\n",
    "        flag = False\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Checked: (0, {i})\")\n",
    "\n",
    "print(\"All columns passed\" if flag == True else \"Not passed\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Clean data:\n",
    "\n",
    "    Data was cleaned previously in 'scratchpad.py' file and now all the records are represented by integers(documentation of each column values is located in 'model.py' file). Values ​​of 0 represent an error in reading data. Column \"Estimation\" contains a lot of 0 values, but this is due to the fact that not every article on the website contained such information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if df.isna().any().any():\n",
    "    df = df.dropna()\n",
    "    print(\"All rows with NaN values were dropped\")\n",
    "else:\n",
    "    print(\"0 NaN values\")\n",
    "\n",
    "cols_to_check = df.columns[df.columns != 'Estimation']\n",
    "if df[cols_to_check].eq(0).any().any():\n",
    "    df[cols_to_check] = df[cols_to_check].replace(0, np.nan)\n",
    "    print(\"All rows with 0 values were dropped\")\n",
    "else:\n",
    "    print(\"0 zero values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Getting know data, analyzing dependencies\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_hist(col_name, bins_val):\n",
    "    min = df[col_name].min()\n",
    "    max = df[col_name].max()\n",
    "    print(f\"Lowest {col_name} value: {min}. Highest {col_name} value: {max}.\")\n",
    "    plot_hist = df[col_name].plot.hist(bins=bins_val, grid=True)\n",
    "    plot_hist.set_title(f\"Represents number of cars for each production {col_name.upper()} category\")\n",
    "    plot_hist.set_xlabel(f\"{col_name}\")\n",
    "    plot_hist.set_ylabel(\"Number of observations\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_hist(\"Price\", 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "min = df[\"Year\"].min()\n",
    "max = df[\"Year\"].max()\n",
    "make_hist(\"Year\", max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_hist(\"Mileage\", 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_hist(\"Tank capacity\", 10) # default bins = 10\n",
    "\n",
    "# Trzeba jeszcze dopasować przedziały"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "make_hist(\"Fuel type\", 10) # default bins = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NO_cars_fuel = df['Fuel type'].value_counts().reset_index()\n",
    "NO_cars_fuel = NO_cars_fuel.rename(columns={'index': 'type of fuel', 'Fuel type': 'NO of cars with specific fuel type'})\n",
    "# NaN\n",
    "# d = {\"Benzyna\": 1, \"Benzyna+LPG\": 2, \"Benzyna+CNG\": 3, \"Elektryczny\": 4, \"Hybryda\": 5, \"Wodór\": 6, \"Diesel\": 7}\n",
    "# unique_values['name of fuel'] = unique_values['type of fuel'].map(d)\n",
    "NO_cars_fuel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "make_hist(\"Model\", 29)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "make_hist(\"Estimation\", 10) # default bins = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Distribution of cars per column per certain category from column in numbers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pr = df['Price'].value_counts().reset_index()\n",
    "# yr = df['Year'].value_counts().reset_index()\n",
    "# mil = df['Mileage'].value_counts().reset_index()\n",
    "# tank = df['Tank capacity'].value_counts().reset_index()\n",
    "# fuel = df['Fuel type'].value_counts().reset_index()\n",
    "mod = df['Model'].value_counts().reset_index()\n",
    "est = df['Estimation'].value_counts().reset_index()\n",
    "\n",
    "# pr = pr.rename(columns={'index': 'price_cat', 'Price': 'NO_cars'})\n",
    "# yr = yr.rename(columns={'index': 'year_cat', 'Year': 'NO_cars'})\n",
    "# mil = mil.rename(columns={'index': 'mileage_cat', 'Mileage': 'NO_cars'})\n",
    "# tank = tank.rename(columns={'index': 'tank_capacity', 'Tank capacity': 'NO_cars'})\n",
    "# fuel = fuel.rename(columns={'index': 'type_fuel', 'Fuel type': 'NO_cars'})\n",
    "mod = mod.rename(columns={'index': 'type_model', 'Model': 'NO_cars'})\n",
    "est = est.rename(columns={'index': 'type_est', 'Estimation': 'NO_cars'})\n",
    "\n",
    "def create_table(table_data, table_title):\n",
    "    fig = plt.figure()\n",
    "    table = plt.table(cellText=table_data.values, colLabels=table_data.columns, loc='upper left')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1, 1.5)\n",
    "    plt.subplots_adjust(left=0.2, bottom=0.2)\n",
    "    plt.title(table_title)\n",
    "    plt.axis('off')\n",
    "    return fig\n",
    "\n",
    "# table1 = create_table(pr, \"Pr\")\n",
    "# table2 = create_table(yr, \"Yr\")\n",
    "# table3 = create_table(mil, \"Mil\")\n",
    "# table4 = create_table(tank, \"Tank\")\n",
    "# table5 = create_table(fuel, \"Fuel\")\n",
    "table6 = create_table(mod, \"Mod\")\n",
    "table7 = create_table(est, \"Est\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Box plots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, column in enumerate(df.columns):\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.boxplot(data=df[column], ax=ax).set(title=f\"{column.upper()} boxplot\", xlabel=f\"{column}\", ylabel=f\"Value of {column}\")\n",
    "    ax.set_title(column)\n",
    "    sns.despine()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Heat Map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correlation_matrix = np.corrcoef(df.values.T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "sns.set(font_scale=1.1)\n",
    "sns.heatmap(data=correlation_matrix, square=True, cbar=True, annot=True, annot_kws={'size': 10}, xticklabels=df.columns, yticklabels=df.columns, fmt=\".2f\", linewidth=.5, cmap=sns.cubehelix_palette(as_cmap=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Outlier detection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/durgeshrao9993/removing-outliers-by-iqr-method\n",
    "\n",
    "# # Interquartile Range (IQR)\n",
    "# Q1 = data.quantile(0.25)\n",
    "# Q3 = data.quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "# data = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Calculate the z-scores of each column - measure that represents the number of standard deviations a data point is from the mean of the dataset\n",
    "z_scores = (df - df.mean()) / df.std()\n",
    "# Set the threshold for the z-score\n",
    "threshold = 3\n",
    "# Remove any rows where the z-score is greater than the threshold\n",
    "df = df[(np.abs(z_scores) < threshold).all(axis=1)]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"14915 dropped to 14105 rows\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Heat Map - after outlier detection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.boxplot(df['Price']).set(title=\"PRICE boxplot\", xlabel=\"Price\", ylabel=\"Value of Price\")\n",
    "sns.despine()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.boxplot(df['Year']).set(title=\"YEAR boxplot\", xlabel=\"Year\", ylabel=\"Value of Year\")\n",
    "sns.despine()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.boxplot(df['Mileage']).set(title=\"MILEAGE boxplot\", xlabel=\"Mileage\", ylabel=\"Value of Mileage\")\n",
    "sns.despine()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.boxplot(df['Tank capacity']).set(title=\"TANK CAPACITY boxplot\", xlabel=\"Tank capacity\", ylabel=\"Value of Tank capacity\")\n",
    "sns.despine()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Splitting data into sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SKLEARN\n",
    "\n",
    "X = df.drop(['Price'], axis=1)\n",
    "y = df['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# KERAS/TENSORFLOW\n",
    "\n",
    "# data_size = len(df)\n",
    "# test_size = int(data_size * 0.2)  # 20% danych przeznaczamy na zbiór testowy\n",
    "# train_size = data_size - test_size\n",
    "#\n",
    "# data = tf.data.Dataset.from_tensor_slices((X.values, y.values))\n",
    "# data = data.shuffle(buffer_size=data_size, reshuffle_each_iteration=True) # randomize data after each iteration (epoch)\n",
    "#\n",
    "# # Helps to optimize code by dividing datasets into batches, portion of data instead of individual processing.\n",
    "# train_data = data.take(train_size).batch(32)\n",
    "# test_data = data.skip(train_size).take(test_size).batch(32)\n",
    "# X_train, y_train = next(iter(train_data))\n",
    "# X_test, y_test = next(iter(test_data))\n",
    "#\n",
    "# # Print length of datasets portions\n",
    "# print(f\"X_train len = {len(X_train)}\")\n",
    "# print(f\"y_train len = {len(y_train)}\")\n",
    "# print(f\"X_test len = {len(X_test)}\")\n",
    "# print(f\"y_test len = {len(y_test)}\")\n",
    "#\n",
    "# # Print datasets portions\n",
    "# # print('X_train = ', X_train)\n",
    "# # print('y_train = ', y_train)\n",
    "# # print('X_test = ', X_test)\n",
    "# # print('y_test = ', y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalization / Scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalization - we change the distribution of data, not the range of data like it is done in scaling. The point is to change your observations so that they can be described as a normal distribution.\n",
    "# Normalization should be done after splitting into train and test data sets!!!\n",
    "# https://keras.io/api/layers/preprocessing_layers/numerical/normalization/\n",
    "# fit_transform() is used to learn and apply the transformation to the data in one step, whereas transform() is used to apply the transformation to new data using the learned parameters.\n",
    "# If you are using a regression model and want to predict a continuous target variable (i.e., y), then you should scale both the input features (X) and the target variable (y). This is because scaling both the input features and target variable will help the model to learn better and converge faster.\n",
    "# If you are using a classification model and want to predict a binary or categorical target variable, then you do not need to scale the target variable (y). This is because scaling does not change the nature of the target variable, and most classification algorithms do not depend on the scale of the target variable.\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_train_scaled = scaler.transform(y_train)\n",
    "y_test_scaled = scaler.transform(y_test)\n",
    "# convert dataframe to numpy array\n",
    "# values = df.values\n",
    "# normalized_values = keras.utils.normalize(values, axis=0)\n",
    "# df_normalized = pd.DataFrame(normalized_values, columns=df.columns)\n",
    "# print(\"Normalization done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pair plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.pairplot(df, height=1.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model Architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/guide/keras/rnn?hl=pl\n",
    "# https://www.ibm.com/topics/recurrent-neural-networks\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=(None, X_train.shape[-1])), # type of recurrent neural network\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Equivalent to:\n",
    "# input_layer = tf.keras.layers.Input(shape=(None, X_train.shape[-1]))\n",
    "# lstm_layer = tf.keras.layers.LSTM(64)(input_layer)\n",
    "# dense_layer_1 = tf.keras.layers.Dense(32, activation='relu')(lstm_layer)\n",
    "# output_layer = tf.keras.layers.Dense(1)(dense_layer_1)\n",
    "# model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compile and train training the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_scaled,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test_scaled, y_test_scaled),\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_loss, train_mse, train_mae, train_r2, train_acc = model.evaluate(X_train_scaled, y_train_scaled, verbose=0)\n",
    "print(\"Training Loss: {:.4f}\".format(train_loss))\n",
    "print(\"Training MSE: {:.4f}\".format(train_mse))\n",
    "print(\"Training MAE: {:.4f}\".format(train_mae))\n",
    "print(\"Training R2: {:.4f}\".format(train_r2))\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_acc))\n",
    "\n",
    "# Evaluate the model on the validation (test) set\n",
    "# \"Generally, the term “validation set” is used interchangeably with the term “test set”\"\n",
    "val_loss, val_mse, val_mae, val_r2, val_acc = model.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "print(\"Validation Loss: {:.4f}\".format(val_loss))\n",
    "print(\"Validation MSE: {:.4f}\".format(val_mse))\n",
    "print(\"Validation MAE: {:.4f}\".format(val_mae))\n",
    "print(\"Validation R2: {:.4f}\".format(val_r2))\n",
    "print(\"Validation Accuracy: {:.4f}\".format(val_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loss and accuracy plots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the loss function\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss function')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the loss and accuracy curves\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the best hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Possible DEPLOY - docker, github"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}